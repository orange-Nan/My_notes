决策树Decision Tree

1.属性：
决策树是一种有监督学习算法（分类或回归）。

2.原理

3.思路：
决策树是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，
而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到
达叶子节点，将叶子节点存放的类别作为决策结果。

4.常用求解方法：
一棵决策树的生成过程主要分为以下3个部分:
特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。
决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。
剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

5.优点：
（预处理）基本不需要预处理，不需要提前归一化
（缺失值）对缺失值不敏感，可以不处理缺失值。对于异常点的容错能力好，健壮性高
（离散值）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
（逻辑）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释
缺点:
（过拟合）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
（算法改变）决策树会因为样本发生一点点的改动（特别是在节点的末梢），导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
（复杂关系）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
（特征比例）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

6.与其他模型对比
