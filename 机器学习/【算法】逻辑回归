逻辑回归Logistic Regression

1.属性：分类模型，常用于二分类

2.原理：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

3.思路：以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。
逻辑回归的思路是，先拟合决策边界（不局限于线性，还可以是多项式），再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。
逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在
这组参数下，我们的数据的似然度（概率）最大。

4.常用求解方法：随机梯度下降、牛顿法、正则化
（1）随机梯度下降：通过 J(w) 对 w 的一阶导数来找下降方向，并且以迭代的方式来更新参数
（2）牛顿法：在现有极小点估计值的附近对 f(x) 做二阶泰勒展开，进而找到极小点的下一个估计值
（3）正则化：正则化是结构风险最小化的一种策略实现，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。
在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微
有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。

5.优点：（1）直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
（2）不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
（3）对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。

6.与其他模型对比
（1）与线性回归
逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于
广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。
我们需要明确 Sigmoid 函数到底起了什么作用：
  线性回归是在实数域范围内进行预测，而分类范围则需要在[0,1]，逻辑回归减少了预测范围；
  线性回归在实数域上敏感度一致，而逻辑回归在0附近敏感，在远离0点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。
（2）与最大熵模型
逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。
