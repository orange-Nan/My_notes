逻辑回归Logistic Regression（LR）

1.属性：分类模型
*应用场景：
用于分类：适合做很多分类算法的基础组件（常用于二分类）
*适用场景：
基本假设：输出类别服从伯努利二项分布
样本线性可分
特征空间不是很大的情况
不必在意特征间相关性的情景
后续会有大量新数据的情况

2.原理：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

3.思路：以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。
逻辑回归的思路是，先拟合决策边界（不局限于线性，还可以是多项式），再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。
逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在
这组参数下，我们的数据的似然度（概率）最大。

4.常用求解方法：随机梯度下降、牛顿法、正则化
（1）随机梯度下降：通过 J(w) 对 w 的一阶导数来找下降方向，并且以迭代的方式来更新参数
（2）牛顿法：在现有极小点估计值的附近对 f(x) 做二阶泰勒展开，进而找到极小点的下一个估计值
（3）正则化：正则化是结构风险最小化的一种策略实现，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。
在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微
有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。

5.优点：
（1）直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
（2）不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
（3）（参数）参数代表每个特征对输出的影响，可解释性强。
（4）（简单高效）实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用。
（5）（可扩展）可以使用online learning的方式更新轻松更新参数，不需要重新训练整个模型。
（6）（过拟合）解决过拟合的方法很多，如L1、L2正则化。
缺点：
（1）（特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。
（2）（特征空间）特征空间很大时，性能不好。
（3）（精度）容易欠拟合，精度不高。

6.与其他模型对比
（1）与线性回归
逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于
广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。
我们需要明确 Sigmoid 函数到底起了什么作用：
  线性回归是在实数域范围内进行预测，而分类范围则需要在[0,1]，逻辑回归减少了预测范围；
  线性回归在实数域上敏感度一致，而逻辑回归在0附近敏感，在远离0点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。
  *鲁棒性：指控制系统在一定的参数摄动（干扰）下，维持其它某些性能的特性
（2）决策树
数据的结构：逻辑回归胜在整体分析，决策树胜在局部分析。
线性特性：逻辑回归擅长线性数据，决策树擅长非线性。
缺失值。
（3）与 SVM
极值：逻辑回归对极值敏感，SVM不敏感。
参数：LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上
构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无
法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是
样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；
概率：LR 可以产生概率，SVM 不能；
（距离）LR 不依赖样本之间的距离，SVM 是基于距离的；
（大数据）LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只
需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。
（4）与朴素贝叶斯
朴素贝叶斯和逻辑回归都属于分类模型，当朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时，它计算出来的 P(Y=1|X) 形式跟逻辑回归是一样的。
两个模型不同的地方在于：
逻辑回归是判别式模型 p(y|x)，朴素贝叶斯是生成式模型 p(x,y)：判别式模型估计的是条件概率分布，给定观测变量 x 和目标变量 y 的条件模型，由
数据直接学习决策函数 y=f(x) 或者条件概率分布 P(y|x) 作为预测的模型。判别方法关心的是对于给定的输入 x，应该预测什么样的输出 y；而生成式
模型估计的是联合概率分布，基本思想是首先建立样本的联合概率概率密度模型 P(x,y)，然后再得到后验概率 P(y|x)，再利用它进行分类，生成式更关
心的是对于给定输入 x 和输出 y 的生成关系；
朴素贝叶斯的前提是条件独立，每个特征权重独立，所以如果数据不符合这个情况，朴素贝叶斯的分类表现就没逻辑回归好了。


