逻辑回归Logistic Regression（LR）

1.属性：分类模型，常用于二分类

2.原理：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

3.思路：以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成线性可分。
逻辑回归的思路是，先拟合决策边界（不局限于线性，还可以是多项式），再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。
逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在
这组参数下，我们的数据的似然度（概率）最大。

4.常用求解方法：随机梯度下降、牛顿法、正则化
（1）随机梯度下降：通过 J(w) 对 w 的一阶导数来找下降方向，并且以迭代的方式来更新参数
（2）牛顿法：在现有极小点估计值的附近对 f(x) 做二阶泰勒展开，进而找到极小点的下一个估计值
（3）正则化：正则化是结构风险最小化的一种策略实现，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。
在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微
有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。

5.优点：（1）直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
（2）不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
（3）对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。

6.与其他模型对比
（1）与线性回归
逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于
广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。
我们需要明确 Sigmoid 函数到底起了什么作用：
  线性回归是在实数域范围内进行预测，而分类范围则需要在[0,1]，逻辑回归减少了预测范围；
  线性回归在实数域上敏感度一致，而逻辑回归在0附近敏感，在远离0点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。
（2）与最大熵模型
逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。
（3）与 SVM
相同点：
都是分类算法，本质上都是在找最佳分类超平面；
都是监督学习算法；
都是判别式模型，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；
都可以增加不同的正则项。
不同点：
LR 是一个统计的方法，SVM 是一个几何的方法；
SVM 的处理方法是只考虑 Support Vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权
重，相对提升了与分类最相关的数据点的权重；
损失函数不同：LR 的损失函数是交叉熵，SVM 的损失函数是 HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类
关系较小的数据点的权重。对 HingeLoss 来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持
向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；
LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上
构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无
法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是
样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；
LR 可以产生概率，SVM 不能；
LR 不依赖样本之间的距离，SVM 是基于距离的；
LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只
需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。
（4）与朴素贝叶斯
朴素贝叶斯和逻辑回归都属于分类模型，当朴素贝叶斯的条件概率P(X|Y=ck)服从高斯分布时，它计算出来的 P(Y=1|X) 形式跟逻辑回归是一样的。
两个模型不同的地方在于：
逻辑回归是判别式模型 p(y|x)，朴素贝叶斯是生成式模型 p(x,y)：判别式模型估计的是条件概率分布，给定观测变量 x 和目标变量 y 的条件模型，由
数据直接学习决策函数 y=f(x) 或者条件概率分布 P(y|x) 作为预测的模型。判别方法关心的是对于给定的输入 x，应该预测什么样的输出 y；而生成式
模型估计的是联合概率分布，基本思想是首先建立样本的联合概率概率密度模型 P(x,y)，然后再得到后验概率 P(y|x)，再利用它进行分类，生成式更关
心的是对于给定输入 x 和输出 y 的生成关系；
朴素贝叶斯的前提是条件独立，每个特征权重独立，所以如果数据不符合这个情况，朴素贝叶斯的分类表现就没逻辑回归好了。
